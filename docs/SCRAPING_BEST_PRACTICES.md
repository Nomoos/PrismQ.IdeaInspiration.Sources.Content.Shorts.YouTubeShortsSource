# YouTube Shorts Scraping Best Practices

## Overview

This document addresses common questions about YouTube Shorts scraping, including:
- How views_per_day is calculated
- Whether to re-scrape videos or download once
- Safety considerations and ban prevention
- Alternative scrapers and downloaders

## How Views Per Day is Calculated

### Current Implementation

The `views_per_day` metric is calculated as follows:

```python
# From src/sources/youtube_channel_plugin.py (lines 297-307)
views_per_day = 0.0
upload_date_str = metadata.get('upload_date')  # Format: YYYYMMDD (e.g., "20231015")

if upload_date_str and view_count > 0:
    upload_date = datetime.strptime(upload_date_str, '%Y%m%d')
    days_since_upload = (datetime.now() - upload_date).days
    
    if days_since_upload > 0:
        views_per_day = view_count / days_since_upload
```

### How It Works

1. **Upload Date**: Extracted from YouTube metadata in YYYYMMDD format
2. **Days Since Upload**: Calculated as `(current_date - upload_date).days`
3. **Views Per Day**: `total_views / days_since_upload`

### Example

```
Video uploaded: October 1, 2023
Current date: October 15, 2023
Days since upload: 14 days
Total views: 100,000
Views per day: 100,000 / 14 = 7,142 views/day
```

### Important Notes

- **Snapshot Metric**: This is a point-in-time calculation based on current view count
- **Average Only**: Represents average performance, not actual daily breakdown
- **No Historical Data**: We don't track how views accumulated over time
- **Recalculated on Re-scrape**: If you re-scrape the same video later, views_per_day will be updated with new data

## Re-Scraping vs. One-Time Download

### Question: Should I Re-Scrape Videos or Download Once?

**Answer: It depends on your use case.**

### Strategy 1: One-Time Collection (Recommended for Most Use Cases) ‚úÖ

**When to use:**
- Building an idea inspiration library
- Content research and analysis
- Studying successful formats
- Training data for ML models

**Advantages:**
- ‚úÖ **Safer**: Minimal risk of rate limiting or bans
- ‚úÖ **Faster**: No repeated downloads
- ‚úÖ **Simpler**: Database deduplication handles duplicates automatically
- ‚úÖ **Efficient**: Lower bandwidth and storage usage
- ‚úÖ **Stable**: Snapshot represents video at specific time

**Implementation:**
```bash
# Scrape once and store
python -m src.cli scrape-channel --channel @channelname --top 50

# Data persists in database with UNIQUE(source, source_id)
# Re-running won't create duplicates, will update existing records
```

**Database Behavior:**
```sql
-- From src/database.py
UNIQUE(source, source_id)
ON CONFLICT(source, source_id) DO UPDATE SET
    -- Updates existing records instead of creating duplicates
```

### Strategy 2: Periodic Re-Scraping (For Tracking Growth)

**When to use:**
- Tracking viral video performance
- Monitoring engagement trends over time
- Studying view velocity patterns
- Competitive analysis

**Advantages:**
- ‚úÖ **Updated Data**: Get current view counts and engagement
- ‚úÖ **Trend Analysis**: Track how videos perform over time
- ‚úÖ **Growth Metrics**: Calculate acceleration/deceleration

**Disadvantages:**
- ‚ö†Ô∏è **Higher Risk**: More requests = higher chance of throttling
- ‚ö†Ô∏è **Slower**: Multiple download sessions
- ‚ö†Ô∏è **Complexity**: Need to store historical snapshots
- ‚ö†Ô∏è **Bandwidth**: Repeated downloads use more resources

**Recommended Frequency:**
- **Conservative**: Weekly (every 7 days)
- **Moderate**: Every 3 days
- **Aggressive**: Daily (higher risk)

**Implementation:**
```python
# Pseudo-code for periodic re-scraping
import time
from datetime import datetime, timedelta

def periodic_rescrape(video_id, min_interval_days=7):
    """Re-scrape video if enough time has passed."""
    last_scrape = get_last_scrape_date(video_id)
    
    if not last_scrape or (datetime.now() - last_scrape) > timedelta(days=min_interval_days):
        # Safe to re-scrape
        scrape_video(video_id)
        update_last_scrape_date(video_id, datetime.now())
        
        # Add delay between requests
        time.sleep(5)  # 5 second delay
```

## Ban Prevention and Safety

### Is Re-Scraping Safe?

**Short Answer: Mostly safe with proper precautions, but one-time collection is safer.**

### Risk Factors

| Factor | Risk Level | Mitigation |
|--------|-----------|------------|
| **Frequency** | üü° Medium | Limit to weekly or less frequent |
| **Volume** | üü° Medium | Max 100-200 videos per session |
| **Speed** | üî¥ High | Always add delays (5-10s between requests) |
| **Pattern** | üü° Medium | Vary timing, don't scrape at exact intervals |
| **IP Address** | üü¢ Low | Use residential IP, avoid datacenter IPs |

### YouTube's Position on Scraping

**Official Stance:**
- YouTube Terms of Service (Section 4.B): "You are not allowed to... access the Service using any automated means (such as robots, botnets or scrapers)"
- However, yt-dlp is widely used and tolerated for personal use
- YouTube primarily targets commercial-scale scraping operations

**Reality:**
- ‚úÖ **Personal Use**: Generally tolerated
- ‚ö†Ô∏è **Research Use**: Gray area, proceed with caution
- ‚ùå **Commercial Use**: High risk of legal action

### Best Practices for Safe Scraping

#### 1. Use yt-dlp (Not YouTube API for Heavy Scraping)

**Why yt-dlp is safer:**
- No API quota (so no "hard limit" violations)
- Uses same endpoints as regular YouTube viewing
- Mimics browser behavior
- Regularly updated to adapt to YouTube changes

**YouTube API Risks:**
- Hard quota limits (10,000 units/day)
- Easier to track and throttle
- Requires registration (traceable)

#### 2. Implement Rate Limiting

```python
import time
import random

def scrape_with_delays(video_ids):
    for video_id in video_ids:
        scrape_video(video_id)
        
        # Random delay between 5-15 seconds
        delay = random.uniform(5, 15)
        print(f"Waiting {delay:.1f}s before next request...")
        time.sleep(delay)
```

#### 3. Respect YouTube's Infrastructure

```python
# Good practices:
- Limit concurrent connections (don't parallelize too much)
- Scrape during off-peak hours (avoid 12pm-3pm UTC)
- Use realistic user-agent strings
- Don't overwhelm specific channels
```

#### 4. Error Handling and Backoff

```python
import time

def scrape_with_retry(video_id, max_retries=3):
    for attempt in range(max_retries):
        try:
            return scrape_video(video_id)
        except Exception as e:
            if "429" in str(e):  # Rate limited
                wait_time = (2 ** attempt) * 60  # Exponential backoff
                print(f"Rate limited. Waiting {wait_time}s...")
                time.sleep(wait_time)
            else:
                raise
    return None
```

### Signs You're Being Throttled

Watch for these indicators:
- ‚ö†Ô∏è **HTTP 429 errors**: Too Many Requests
- ‚ö†Ô∏è **Slow response times**: Unusual delays
- ‚ö†Ô∏è **Incomplete metadata**: Missing fields in responses
- ‚ö†Ô∏è **CAPTCHA challenges**: Asked to verify you're human

**If throttled:**
1. Stop scraping immediately
2. Wait 24-48 hours
3. Reduce frequency/volume when resuming
4. Implement longer delays

### Recommended Scraping Schedule

#### Conservative (Safest) ‚≠ê Recommended

```
Initial scrape: Collect all videos once
Re-scrape frequency: Every 14 days (for trend tracking)
Daily limit: 50-100 videos
Delay between requests: 10-15 seconds
Risk level: üü¢ Very Low
```

#### Moderate (Balanced)

```
Initial scrape: Collect all videos once
Re-scrape frequency: Every 7 days
Daily limit: 100-200 videos
Delay between requests: 5-10 seconds
Risk level: üü° Low-Medium
```

#### Aggressive (Higher Risk)

```
Initial scrape: Collect all videos once
Re-scrape frequency: Every 3 days
Daily limit: 200-300 videos
Delay between requests: 3-5 seconds
Risk level: üî¥ Medium-High
```

## Alternative Scrapers and Downloaders

### Current: yt-dlp (Recommended) ‚≠ê

**Pros:**
- ‚úÖ Actively maintained (weekly updates)
- ‚úÖ Comprehensive metadata extraction
- ‚úÖ Handles YouTube changes quickly
- ‚úÖ Large community (1M+ GitHub stars)
- ‚úÖ Subtitle support
- ‚úÖ Quality metrics (resolution, FPS)

**Cons:**
- ‚ö†Ô∏è Can break when YouTube updates
- ‚ö†Ô∏è Requires Python installation
- ‚ö†Ô∏è Slower than API for bulk collection

**Repository:** https://github.com/yt-dlp/yt-dlp

### Alternative 1: youtube-dl (Not Recommended)

**Status:** ‚ö†Ô∏è Legacy, less actively maintained

**Pros:**
- ‚úÖ Mature codebase
- ‚úÖ Similar API to yt-dlp

**Cons:**
- ‚ùå Slower update cycle
- ‚ùå Often broken during YouTube updates
- ‚ùå yt-dlp is a better-maintained fork

**Recommendation:** Use yt-dlp instead

### Alternative 2: YouTube Data API v3

**Status:** ‚úÖ Official, stable

**What it provides:**
- ‚úÖ Title, description, tags
- ‚úÖ Views, likes, comments, favorites
- ‚úÖ Channel info, upload date, categories

**What it's missing:**
- ‚ùå Subtitles (no text extraction)
- ‚ùå Quality metrics (no resolution, FPS, aspect ratio)
- ‚ùå Enhanced analytics (calculated metrics)

**Pros:**
- ‚úÖ Official Google API
- ‚úÖ Guaranteed stability
- ‚úÖ Fast for small queries
- ‚úÖ Well-documented

**Cons:**
- ‚ùå Quota limits (10,000 units/day)
- ‚ùå No subtitles or quality metrics
- ‚ùå No trending access

**When to use:**
- Small-scale queries (<1,000 videos)
- Need guaranteed uptime
- Official data requirements
- Don't need subtitles or quality metrics

### Alternative 3: Invidious API

**Status:** üü° Community-run, variable reliability

**What it is:** Privacy-focused YouTube frontend with API

**Pros:**
- ‚úÖ No Google account needed
- ‚úÖ JSON API
- ‚úÖ No quotas

**Cons:**
- ‚ùå Dependent on instance availability
- ‚ùå Inconsistent uptime
- ‚ùå May lack latest features
- ‚ùå Legal gray area

**Instances:** https://api.invidious.io/

**Example:**
```bash
curl "https://invidious.instance.com/api/v1/videos/VIDEO_ID"
```

### Alternative 4: pytube (Not Recommended for Scraping)

**Status:** ‚ö†Ô∏è Focused on downloads, not metadata

**Pros:**
- ‚úÖ Pure Python
- ‚úÖ Good for downloading videos

**Cons:**
- ‚ùå Limited metadata extraction
- ‚ùå Less maintained than yt-dlp
- ‚ùå Frequently broken

**Recommendation:** Use yt-dlp instead

### Alternative 5: YouTube RSS Feeds

**Status:** ‚úÖ Lightweight, safe for monitoring

**What it is:** YouTube's official RSS feeds for channels

**Pros:**
- ‚úÖ Official and stable
- ‚úÖ Very lightweight
- ‚úÖ No scraping concerns
- ‚úÖ Real-time updates

**Cons:**
- ‚ùå Limited metadata (title, description, thumbnail only)
- ‚ùå No view counts or engagement data
- ‚ùå No quality metrics

**URL Format:**
```
https://www.youtube.com/feeds/videos.xml?channel_id=CHANNEL_ID
```

**When to use:**
- Monitoring new uploads
- Building watch lists
- Feed aggregation

### Comparison Table

| Tool | Metadata | Speed | Stability | Quotas | Risk | Recommendation |
|------|----------|-------|-----------|--------|------|----------------|
| **yt-dlp** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚àû | üü¢ Low | ‚úÖ Best choice |
| **YouTube API** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 10K/day | üü¢ None | Small queries |
| **youtube-dl** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚àû | üü¢ Low | ‚ùå Use yt-dlp |
| **Invidious** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚àû | üü° Medium | Experimental |
| **pytube** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚àû | üü¢ Low | ‚ùå Use yt-dlp |
| **RSS Feeds** | ‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚àû | üü¢ None | Monitoring only |

## Recommendations Summary

### For Idea Collection (Most Users) ‚≠ê

**Strategy:**
1. Use yt-dlp (current implementation)
2. Download videos once
3. Store in database (automatic deduplication)
4. No re-scraping needed

**Why:**
- Safest approach
- Meets 95% of use cases
- Simple to implement
- Low risk

### For Trend Tracking (Advanced Users)

**Strategy:**
1. Use yt-dlp (current implementation)
2. Initial scrape: Collect all videos
3. Re-scrape: Weekly (14 days for safety)
4. Implement delays: 10-15 seconds between requests
5. Daily limit: 50-100 videos maximum

**Implementation checklist:**
- [ ] Add delay between requests (10-15s)
- [ ] Track last scrape date for each video
- [ ] Implement exponential backoff on errors
- [ ] Monitor for throttling indicators
- [ ] Store historical snapshots in separate table

### Best Practices Checklist

- [ ] Use yt-dlp as primary scraper
- [ ] Implement random delays (5-15s) between requests
- [ ] Limit daily volume (50-200 videos max)
- [ ] Add exponential backoff on errors
- [ ] Monitor for HTTP 429 errors
- [ ] Scrape during off-peak hours
- [ ] Use realistic user-agent strings
- [ ] Store last scrape timestamp
- [ ] Implement database deduplication
- [ ] Log all scraping activities

## Code Examples

### Safe Re-Scraping Implementation

```python
import time
import random
from datetime import datetime, timedelta
from src.database import Database
from src.sources.youtube_channel_plugin import YouTubeChannelPlugin

def safe_rescrape(video_ids, min_interval_days=14, delay_range=(10, 15)):
    """Safely re-scrape videos with rate limiting.
    
    Args:
        video_ids: List of video IDs to re-scrape
        min_interval_days: Minimum days between re-scrapes (default: 14)
        delay_range: (min, max) delay in seconds between requests
    """
    db = Database('ideas.db')
    
    for i, video_id in enumerate(video_ids, 1):
        # Check last scrape date
        idea = db.get_idea('youtube_channel', video_id)
        
        if idea:
            last_update = datetime.fromisoformat(idea['updated_at'])
            days_since = (datetime.now() - last_update).days
            
            if days_since < min_interval_days:
                print(f"[{i}/{len(video_ids)}] Skipping {video_id}: Last scraped {days_since} days ago")
                continue
        
        # Scrape with delay
        print(f"[{i}/{len(video_ids)}] Re-scraping {video_id}...")
        try:
            # Your scraping logic here
            metadata = scrape_video(video_id)
            
            if metadata:
                # Update database
                db.insert_idea(...)  # Updates existing record
                print(f"  ‚úì Updated successfully")
            
            # Random delay between requests
            delay = random.uniform(*delay_range)
            print(f"  Waiting {delay:.1f}s before next request...")
            time.sleep(delay)
            
        except Exception as e:
            print(f"  ‚úó Error: {e}")
            if "429" in str(e):
                print("  ‚ö†Ô∏è Rate limited! Stopping...")
                break
    
    db.close()
```

### Tracking Historical Data

```python
# Extend database schema to track history
def create_history_table():
    """Create table for historical snapshots."""
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS idea_history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            source TEXT NOT NULL,
            source_id TEXT NOT NULL,
            view_count INTEGER,
            like_count INTEGER,
            comment_count INTEGER,
            views_per_day REAL,
            engagement_rate REAL,
            scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY(source, source_id) REFERENCES ideas(source, source_id)
        )
    """)

def save_snapshot(source, source_id, metrics):
    """Save historical snapshot of metrics."""
    cursor.execute("""
        INSERT INTO idea_history 
        (source, source_id, view_count, like_count, comment_count, 
         views_per_day, engagement_rate)
        VALUES (?, ?, ?, ?, ?, ?, ?)
    """, (source, source_id, metrics.view_count, metrics.like_count,
          metrics.comment_count, metrics.views_per_day, metrics.engagement_rate))
```

## Conclusion

**Key Takeaways:**

1. **views_per_day** is calculated by dividing total views by days since upload
2. **One-time collection** is recommended for most use cases (safer, simpler)
3. **Re-scraping** should be done weekly at most, with 10-15s delays
4. **yt-dlp** is the best tool (already implemented)
5. **Safety first**: Implement delays, monitor for throttling, respect YouTube's infrastructure

**Next Steps:**

1. Use current implementation (yt-dlp) for one-time collection
2. Only implement re-scraping if you need trend tracking
3. If re-scraping, follow the "Conservative" schedule (weekly, 50-100 videos/day)
4. Monitor for any throttling indicators
5. Consider creating a separate table for historical snapshots if tracking trends

---

**Last Updated:** 2025-10-13  
**Related Documentation:**
- [DATA_COLLECTION_ANALYSIS.md](DATA_COLLECTION_ANALYSIS.md) - What data we collect
- [METRICS.md](METRICS.md) - Metrics documentation
- [README.md](../README.md) - Main project documentation
